---
layout: post
title: 你告诉AI机器人一个“犯罪秘密”，你会被逮捕吗？
date: 2025-11-12 08:37:01.000000000 +00:00
link: https://cn.nytimes.com/opinion/20251112/chatbot-conversations-legal-protection/
categories: nyt
---

<address>NILS GILMAN</address><time pudate="2025-11-12 04:01:32" datetime="2025-11-12 04:01:32">2025年11月12日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2025/11/10/opinion/10gilman/10gilman-master1050.png"><figcaption> <cite>Gaia Alari</cite></figcaption></figure><section><p>元旦那天，乔纳森·林德克内希特据称<a href="https://www.nytimes.com/2025/10/08/us/palisades-fire-arrest-jonathan-rinderknecht.html">问了</a>ChatGPT这样一个问题：“如果因为你的香烟引起了火灾，这算是你的错吗？”ChatGPT回答说：“是的。”10个月后，他现在被指控纵火——当局称他引发了一场小火，火在一周后复燃，最终酿成帕利塞兹特大火。</p><p>根据联邦起诉书，此前林德克内希特曾向聊天机器人透露数月前焚烧圣经带来的“奇妙”感受，还要求其创作一幅“反乌托邦”画作：描绘一群穷人逃离森林大火时，一群富人隔门嘲讽的场景。目前被告已作无罪抗辩。</p><p>联邦当局认为，这些与人工智能的互动印证了林德克内希特的纵火心理、动机与意图。结合将其定位在初始火源现场的GPS数据，足以将其<a href="https://www.nytimes.com/2025/10/08/us/los-angeles-palisades-fire-arrest.html?searchResultPosition=2">逮捕并提出多项指控</a>，包括纵火破坏财产。</p><p>这一令人不安的进展为司法体系敲响警钟。随着越来越多的人将AI聊天工具视为知己、心理治疗师和顾问，我们亟需建立新型法律保护机制，守护人机对话的隐私屏障。我称其为“AI交互特权”。</p><p>所有法律特权都基于同一理念：律师与客户、医生与患者、神父与忏悔者等特定关系，依赖坦诚交流才能促进整个社会的福祉。若无隐私保障，人们便会自我审查，而社会也将失去诚实带来的益处。弗吉尼亚大学法学教授格雷格·米切尔告诉我，法院历来不愿创设新特权，除非“保密性成为维系该关系的绝对必要条件”。如今，众多用户与AI的交互已达到了这一门槛。</p><p>人们越来越自然地与人工智能系统交谈，不再把它用作日记，而是视其为对话伙伴。因为这些系统进行的对话已与人类交流无异。机器似乎在倾听、推理、回应——在某些情况下，它不仅反映用户的思想和情感，甚至对其加以塑造。人工智能系统能像优秀的律师或心理治疗师一样引导人们表达自己。许多人之所以求助于人工智能，正是因为他们缺乏一个安全且负担得起的人类对象来倾诉禁忌或脆弱的想法。</p><p>这在某种程度上可以说是设计使然。就在上个月，OpenAI首席执行官萨姆·奥尔特曼<a rel="noopener noreferrer" target="_blank" href="https://x.com/sama/status/1978129344598827128">宣布</a>，其ChatGPT平台的下一代版本将“放宽”部分用户限制，允许用户让自己的ChatGPT“以高度人性化的方式回应”。</p><p>让政府获取这些未经筛选的交流内容并将其视为法律供词，将产生巨大的寒蝉效应。如果每个私密的思想实验日后都可能成为法庭上的武器，AI用户必将自我审查，从而削弱这些系统一些最具价值的功能。这将摧毁人工智能在心理健康、法律和财务问题的解决过程中所依赖的那种坦诚关系，把这种本可成为自我探索与自我表达的强大工具变成潜在的法律风险。</p><p>目前，大多数数字互动都属于“第三方原则”的范畴。该原则认为，任何自愿向第三方披露的信息——或存储在企业服务器上的数据——都“不具备合法的隐私期待”。这使政府能够在无需搜查令的情况下获取大量线上行为记录（例如谷歌搜索历史）。</p><p>但AI对话是否属于此种意义上的"自愿披露"？既然众多用户将这些系统视为私人顾问而非搜索引擎，法律标准就应当与时俱进，以反映这种保密期待。AI企业掌握的私人数据量，已超越任何心理治疗师或律师所能触及的私密范畴，至今却未承担明确的法律保护责任。</p><p>“AI交互特权”应在三个方面借鉴现有的法律特权。首先，为寻求咨询或情绪疏导而与人工智能进行的交流，应受到保护，免于在法庭上被强制披露。用户可以通过应用程序的设置来指定受保护的会话，或在法律取证阶段主张特权，只要对话的上下文支持这一主张。其次，该特权必须纳入所谓的“警示义务”原则，即AI服务合理判定认为当事人对自己或他人构成迫在眉睫的威胁或者已造成实际伤害时，有义务报告。第三，必须为犯罪和欺诈行为设立例外。若AI被用于策划或实施犯罪，相关对话记录应在司法监督下作为证据调取。</p><p>按照这种逻辑，林德克内希特的案件同时揭示了这种保护的必要性与局限性。他关于香烟引发火灾的提问，本质上等同于一次互联网搜索，不应享有特权保护；但根据“AI交互特权”，他关于焚烧《圣经》的坦白则应受到保护——那既不是犯罪计划，也不是迫在眉睫的威胁。</p><p>建立一种新的特权符合法律不断适应新型信任关系的演进模式。心理治疗师与病人之间的保密特权本身直到1996年才被正式承认，当时美国最高法院认定保密对心理治疗具有重要价值。同样的逻辑如今也适用于AI：坦诚交互带来的社会效益，远超过偶尔失去某些证据的代价。</p><p>若放任这些人机对话处于法律真空，将导致公民终日担忧数字化自省可能某天成为呈堂证供。无论是与律师、治疗师，还是与机器的私密交流——都必须享有免于恐惧国家窥探的自由。</p></section><footer><p>Nils Gilman是贝格鲁恩研究所的高级顾问，也是《一颗谦逊之星的孩子：危机时代的行星思维》一书的合著者。</p><p>翻译：杜然</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2025/11/10/opinion/chatbot-conversations-legal-protection.html">点击查看本文英文版。</a></p></footer>
